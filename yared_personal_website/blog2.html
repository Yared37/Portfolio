<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Automation of Taste - Yared Atrsaw</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://unpkg.com/lucide@latest/dist/umd/lucide.js"></script>
</head>
<body>
    <!-- Header -->
    <header class="bg-black text-white p-6">
        <div class="max-w-6xl mx-auto flex items-center justify-between">
            <button onclick="goBack()" class="back-button">
                <i data-lucide="arrow-left" class="w-5 h-5"></i>
                <span>Back to Portfolio</span>
            </button>
            <h1 class="text-2xl font-bold">Blog Post</h1>
        </div>
    </header>

    <!-- Blog Content -->
    <main class="bg-white min-h-screen">
        <div class="max-w-4xl mx-auto px-6 py-12">
            <!-- Blog Header -->
            <div class="mb-12">
                <h1 class="text-4xl font-bold text-gray-900 mb-4">The Automation of Taste</h1>
                <div class="flex items-center gap-4 text-sm text-gray-500 mb-6">
                    <span class="flex items-center gap-1">
                        <i data-lucide="calendar" class="w-4 h-4"></i>
                        January 10, 2025
                    </span>
                    <span>•</span>
                    <span>10 min read</span>
                </div>
                <div class="flex flex-wrap gap-2 mb-8">
                    <span class="px-3 py-1 bg-emerald-100 text-emerald-700 text-sm font-medium">Technology</span>
                    <span class="px-3 py-1 bg-emerald-100 text-emerald-700 text-sm font-medium">Culture</span>
                    <span class="px-3 py-1 bg-emerald-100 text-emerald-700 text-sm font-medium">Algorithms</span>
                </div>
            </div>

            <!-- Blog Content -->
            <div class="prose prose-lg max-w-none">
                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Raise your hand if you've ever said, "I don't even like this song, but it keeps popping up."
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Here's the thing. You didn't stumble upon that track by chance. Algorithms are playing a long game with you. They notice what you linger on, what you skip, what you scroll past, and they start rewriting your preferences one recommendation at a time.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Take a moment. Scroll through your playlist or queue. Do you actually love all of it, or has someone else — a complex web of neural networks — nudged you into liking it?
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    It's not sinister. It's fascinating. You might even find yourself liking things you never thought you would. Or maybe it's terrifying. You're discovering that your "unique taste" is partially borrowed, partially curated, partially grown inside a machine that knows you better than your best friend.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Spotify has this thing they call "taste profiles." They broke it down in a 2019 blog post about their recommendation algorithm. They don't just track what you listen to. They track how you listen. Do you skip the intro? Do you replay the bridge? Do you listen in the morning or late at night? All of that gets fed into a model that predicts what you'll want next.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    But prediction and preference start to blur.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    The mere-exposure effect is a psychology term from the 1960s. Robert Zajonc proved that people develop preferences for things simply because they're familiar with them. You don't need to consciously like something. You just need to see it or hear it enough times. Your brain starts to categorize it as safe, known, and therefore good.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Now apply that to algorithms that can show you the same song five different ways across five different playlists. You're getting mere-exposed into liking things.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    There's this great scene in <em>The Social Dilemma</em> where the engineers talk about recommendation systems. One of them says something like, "We're not showing you what you want. We're training you to want what we're showing you." The system optimizes for engagement, which means keeping you listening, clicking, scrolling. Not making you happy. Not expanding your horizons. Just keeping you on the platform.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Netflix has openly admitted this. Their VP of Product once said in an interview with Wired that their goal is to "help you find something to watch within 90 seconds or you'll lose interest and go do something else." So the algorithm isn't curating based on quality or your actual deep preferences. It's curating based on what will make you click fast.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    For me, this raises bigger questions about culture and human learning. How much of what we claim as "ours" is shaped, quietly, by systems outside ourselves?
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Music journalist Liz Pelly wrote this devastating piece for <em>The Baffler</em> called "Streambait Pop." She investigated how independent musicians were literally creating songs designed to please Spotify's algorithm. Songs with certain BPM ranges. Lyrics that hit emotional keywords the algorithm rewards. Arrangements that avoid skip-inducing lulls. The algorithm isn't just recommending music anymore. It's shaping what gets made.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Same thing is happening in other media. YouTubers talk about "the algorithm" like it's a deity they have to appease. Thumbnail colors that pop. Video lengths that maximize ad breaks. Content that hooks in the first eight seconds or you're done. The algorithm has aesthetic preferences, and creators are conforming to them.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    TikTok has perfected this. The For You Page is so good at predicting what you'll watch that users joke about being "algorithmically programmed." You watch one video about cottage cheese recipes and suddenly your entire feed is cottage cheese. But you keep watching. And after a week, you genuinely think you're a cottage cheese person now. Are you? Or did you just get recommendation-pilled into a new identity?
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Eli Pariser wrote about this back in 2011 in <em>The Filter Bubble</em>. He was worried about how algorithms would trap us in bubbles of our own preferences. But he was thinking about news and politics. He didn't realize it would extend to aesthetics, to taste, to the very building blocks of cultural identity.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    The ability to notice that, to step back and question, is something worth cultivating. Because right now, most people don't even realize it's happening.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    There's a Reddit community called r/SpotifyBypass where people share techniques for breaking out of their algorithmic bubbles. One method is to create entirely new accounts and listen to random albums from different decades and genres. People report it feeling like "cultural detox." Like they'd been living in a narrow tunnel and suddenly remembered the world is huge.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    What would happen if you turned off all recommendations for a month? Just listened to albums front-to-back that you picked randomly from a record store or a friend's collection. Watched movies you'd never heard of with no star ratings. Read books with zero reviews.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Would you discover that your taste is actually different from what the feeds told you it was? Or would you feel lost without the guidance? Both answers are kind of unsettling.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Walter Benjamin wrote this essay in 1935 called "The Work of Art in the Age of Mechanical Reproduction." He was thinking about how mass production would change how we relate to art. But maybe we're in a new era now. The age of algorithmic curation. Where your relationship to culture is mediated by systems optimizing for engagement metrics you don't understand.
                </p>

                <p class="text-lg text-gray-700 leading-relaxed mb-6">
                    Next time you hear a song and think "Wow, I love this," ask yourself: did you choose it, or did it choose you?
                </p>
            </div>
        </div>
    </main>

    <script>
        function goBack() {
            window.history.back();
        }
        
        // Initialize Lucide icons
        lucide.createIcons();
    </script>
</body>
</html>
